model:
  target: model.gaussian_diffusion.Diffusion_TS
  params:
    seq_length: 1024
    feature_size: 24
    timesteps: 128
    sampling_timesteps: 128
    loss_type: 'l2'
    beta_schedule: 'linear'
    label_dim: 30

  backbone:
    target: model.transformer.Transformer
    params:
        n_layer_enc: 4
        n_layer_dec: 4
        d_model: 64
        n_head : 4
        mlp_hidden_times: 4
        attn_pdrop: 0.6
        resid_pdrop: 0.6
        kernel_size: 5
        padding_size: 2



solver:
  base_lr: 1.0e-5
  max_epochs: 4000
  results_folder: ./checkpoint_4_4_64_100_l2_cosine
  gradient_accumulate_every: 2
  save_cycle: 100  # max_epochs // 10
  ema:
    decay: 0.99
    update_interval: 10
  
  scheduler:
    target: lr_sch.ReduceLROnPlateauWithWarmup
    params:
      factor: 0.5
      patience: 200
      min_lr: 1.0e-5
      threshold: 1.0e-1
      threshold_mode: rel
      warmup_lr: 8.0e-4
      warmup: 100 
      verbose: False

dataset:
  batch_size: 8
  samples:
    num_sample: 1000
    size_every: 10


csv_fields: 
 - n_layer_enc
 - n_layer_dec
 - d_model
 - n_head
 - mlp_hidden_times
 - timesteps
 - loss_type
 - beta_schedule
 - lr
 - epoch
 - attn_pdrop
 - resid_pdrop
 - loss
 - time